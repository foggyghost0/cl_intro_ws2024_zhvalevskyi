{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dgromann/cl_intro_ws2024/blob/main/exercises/HomeExercise1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4_fSCGEAFZ3"
   },
   "source": [
    "# Home Exericse 1: Preprocessing and NER\n",
    "In this first home exercise, you will use the knowledge from Tutorial 1 and Tutorial 2 to perform some preprocessing and NLP steps on a news article of your choice. An example article in English is provided in this notebook.\n",
    "\n",
    "In this notebook, please complete all instructions starting with üëã ‚öí in the code cell after the sign or provide your analysis in the text cell after the sign.\n",
    "\n",
    "We will use the newspaper libabry to facilitate the scraping of the news article from a webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kxwz_cPyW4lG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting newspaper3k\n",
      "  Downloading newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from newspaper3k) (4.12.3)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from newspaper3k) (10.4.0)\n",
      "Requirement already satisfied: PyYAML>=3.11 in /opt/anaconda3/lib/python3.12/site-packages (from newspaper3k) (6.0.1)\n",
      "Requirement already satisfied: cssselect>=0.9.2 in /opt/anaconda3/lib/python3.12/site-packages (from newspaper3k) (1.2.0)\n",
      "Requirement already satisfied: lxml>=3.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from newspaper3k) (5.2.1)\n",
      "Requirement already satisfied: nltk>=3.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from newspaper3k) (3.9.1)\n",
      "Requirement already satisfied: requests>=2.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from newspaper3k) (2.32.3)\n",
      "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
      "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: tldextract>=2.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from newspaper3k) (5.1.2)\n",
      "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
      "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting jieba3k>=0.35.1 (from newspaper3k)\n",
      "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.3 in /opt/anaconda3/lib/python3.12/site-packages (from newspaper3k) (2.9.0.post0)\n",
      "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
      "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.5)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.12/site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
      "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: click in /opt/anaconda3/lib/python3.12/site-packages (from nltk>=3.2.1->newspaper3k) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.12/site-packages (from nltk>=3.2.1->newspaper3k) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from nltk>=3.2.1->newspaper3k) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from nltk>=3.2.1->newspaper3k) (4.66.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.10.0->newspaper3k) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.10.0->newspaper3k) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.10.0->newspaper3k) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.10.0->newspaper3k) (2024.8.30)\n",
      "Requirement already satisfied: requests-file>=1.4 in /opt/anaconda3/lib/python3.12/site-packages (from tldextract>=2.0.1->newspaper3k) (1.5.1)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /opt/anaconda3/lib/python3.12/site-packages (from tldextract>=2.0.1->newspaper3k) (3.13.1)\n",
      "Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
      "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
      "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13539 sha256=5268a3dad1d05fd45b35146cba282bfeaee28cce8a9c6e2e0757a1e063a6b0ee\n",
      "  Stored in directory: /Users/bohdan/Library/Caches/pip/wheels/a5/91/9f/00d66475960891a64867914273fcaf78df6cb04d905b104a2a\n",
      "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3342 sha256=4fd9cf6ba2bd62af7000513181808330d0feb24a3b52c29330fcb07258c6e665\n",
      "  Stored in directory: /Users/bohdan/Library/Caches/pip/wheels/9f/9f/fb/364871d7426d3cdd4d293dcf7e53d97f160c508b2ccf00cc79\n",
      "  Building wheel for jieba3k (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398379 sha256=29e454b705bb565103b27fa83ec11f6b059b98c9fb1deb301ac8cb5807059f93\n",
      "  Stored in directory: /Users/bohdan/Library/Caches/pip/wheels/26/72/f7/fff392a8d4ea988dea4ccf9788599d09462a7f5e51e04f8a92\n",
      "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=97715fa7e111d276b7bdfcaf1cc202079955187ad014a19f0fc9ec7190ef4fc5\n",
      "  Stored in directory: /Users/bohdan/Library/Caches/pip/wheels/03/f5/1a/23761066dac1d0e8e683e5fdb27e12de53209d05a4a37e6246\n",
      "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
      "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, feedparser, feedfinder2, newspaper3k\n",
      "Successfully installed feedfinder2-0.0.4 feedparser-6.0.11 jieba3k-0.35.1 newspaper3k-0.2.8 sgmllib3k-1.0.0 tinysegmenter-0.3\n",
      "Collecting lxml_html_clean\n",
      "  Downloading lxml_html_clean-0.4.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: lxml in /opt/anaconda3/lib/python3.12/site-packages (from lxml_html_clean) (5.2.1)\n",
      "Downloading lxml_html_clean-0.4.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: lxml_html_clean\n",
      "Successfully installed lxml_html_clean-0.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install newspaper3k\n",
    "!pip install lxml_html_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SRiR8WeRXVOm",
    "outputId": "8b2c43ba-42c6-42d2-f6df-9facdc27a76e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authors:  ['Oscar Holland'] \n",
      "\n",
      "Title:  Maurizio Cattelan‚Äôs viral banana artwork ‚ÄòComedian‚Äô could now be worth $1.5 million \n",
      "\n",
      "Text of article: \n",
      " CNN ‚Äî\n",
      "\n",
      "When a banana duct-taped to a wall sold for $120,000 in 2019, social media uproar and an age-old debate about the meaning of art ensued.\n",
      "\n",
      "But artist Maurizio Cattelan‚Äôs viral creation, titled ‚ÄúComedian,‚Äù may yet prove a sound investment: On Friday, auction house Sotheby‚Äôs announced that one of the artwork‚Äôs three ‚Äúeditions‚Äù is going back on sale ‚Äî this time with an estimate of $1 million to $1.5 million.\n",
      "\n",
      "For their money, the winning bidder will receive a roll of duct tape and one banana, as well as a certificate of authenticity and official instructions for installing the work. Sotheby‚Äôs confirmed to CNN that neither the tape nor, thankfully, the banana are the originals.\n",
      "\n",
      "‚Äú‚ÄòComedian‚Äô is a conceptual artwork, and the actual physical materials are replaced with every installation,‚Äù an auction spokesperson said via email.\n",
      "\n",
      "Cattelan and French art gallery Perrotin made headlines around the world five years ago when they displayed ‚ÄúComedian‚Äù with a six-figure asking price at the Art Basel Miami Beach fair. The original was created using a banana bought in a Miami grocery store, though the gallery said it could be replaced, as per the artist‚Äôs instructions.\n",
      "\n",
      "The art world was split on the work‚Äôs merits, though some critics saw it as rooted in the rich tradition of conceptual works ‚Äî dating back to Marcel Duchamp‚Äôs famous mounted urinal ‚Äî that question the value of art itself. Crowds soon formed, with fair attendees lining up to see the viral installation.\n",
      "\n",
      "Video Ad Feedback Related video: Why is art so expensive? 02:42 - Source: CNN\n",
      "\n",
      "Events took an unexpected turn when performance artist David Datuna grabbed the banana from the wall, before peeling and eating it in front of hundreds of stunned fair attendees. He later defended the move as an artistic performance in its own right, not an act of vandalism.\n",
      "\n",
      "The Miami installation was eventually removed amid public safety concerns, but all three editions were sold at the fair. Two were bought by private collectors for $120,000, while the third was purchased for a higher (but undisclosed) sum, and was later donated to The Guggenheim museum in New York.\n",
      "\n",
      "Sotheby‚Äôs would not reveal the identity of the seller in November‚Äôs auction, but said the work‚Äôs current owner had acquired it from the collection of one of the original buyers.\n",
      "\n",
      "In interviews given since the Miami installation, Cattelan has described ‚ÄúComedian‚Äù as a work of commentary. Speaking to the Art Newspaper in 2021 he said it was ‚Äúnot a joke,‚Äù calling the viral installation ‚Äúa reflection on what we value.‚Äù\n",
      "\n",
      "The Italian artist, who is known for satirical pieces that challenge popular culture, did not immediately respond to CNN‚Äôs request for comment about November‚Äôs sale.\n",
      "\n",
      "An installation shot of \"Comedian\" released by Sotheby's auction house ahead of the sale. courtesy Sotheby‚Äôs\n",
      "\n",
      "In a press release announcing the sale, Sotheby‚Äôs head of contemporary art for the Americas, David Galperin, described ‚ÄúComedian‚Äù as a ‚Äúdefiant work of pure genius.‚Äù\n",
      "\n",
      "‚ÄúBalancing profound critical thought and subversive wit, this is a defining work for the artist and for our generation,‚Äù Galperin said, adding: ‚ÄúIf at its core, ‚ÄòComedian‚Äô questions the very notion of the value of art, then putting the work at auction‚Ä¶ will be the ultimate realization of its essential conceptual idea ‚Äî the public will finally have a say in deciding its true value.‚Äù\n",
      "\n",
      "While this marks the first time ‚ÄúComedian‚Äù will appear at auction, the work was recently exhibited at the Leeum Museum of Art in Seoul, South Korea. It was eaten then, too: An art student from Seoul National University removed the fruit and devoured it, before taping the peel back to the wall.\n",
      "\n",
      "‚ÄúThe student told the museum he ate it because he was hungry,‚Äù a gallery spokesperson told CNN after the 2023 incident. The museum later replaced the eaten banana with a fresh one.\n",
      "\n",
      "Sotheby‚Äôs intends to exhibit the artwork ahead of the sale, which takes place at the auction house‚Äôs New York headquarters on November 20. ‚ÄúComedian‚Äù will be put on show there on Monday before it embarks on a world tour stopping in London, Paris, Milan, Hong Kong, Dubai, Taipei, Tokyo and Los Angeles.\n"
     ]
    }
   ],
   "source": [
    "import newspaper\n",
    "from newspaper import Article\n",
    "\n",
    "url = 'https://edition.cnn.com/2024/10/25/style/banana-artwork-maurizio-cattelan-comedian-auction/index.html'\n",
    "article = Article(url)\n",
    "article.download()\n",
    "article.parse()\n",
    "\n",
    "#This line displays the authors of the article\n",
    "print(\"Authors: \", article.authors, \"\\n\")\n",
    "\n",
    "#This line displays the title and entire text of the article\n",
    "print(\"Title: \", article.title, \"\\n\")\n",
    "print(\"Text of article: \\n\", article.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3xklUvEXbd1"
   },
   "source": [
    "üëã ‚öí Use the above article or a news article of your choice and print the number of unique words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "knGLoU0kXaZb"
   },
   "outputs": [],
   "source": [
    "# Calculate and print the number of unique words in the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DMEt4uoXgFG"
   },
   "source": [
    "## **Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZjKWTTZX3m-"
   },
   "source": [
    "üëã ‚öí Now perform the following preprocessing steps and see how the number of unique words changes:\n",
    "\n",
    "1. Lowercase all words in the text.\n",
    "2. Remove punctuation markers and numbers (Hint: `string.isalpha()).\n",
    "3. Lemmatize all words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-p7kAp-ZYNz7"
   },
   "outputs": [],
   "source": [
    "# Preprocess the text with all three steps and then calculate the number of\n",
    "# unique words in the text again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F81suSVkaS1v"
   },
   "source": [
    "## **NER**\n",
    "\n",
    "In the tutorial we have only used one of the different models available in spaCy. In this exercise, you will compare the performance of the different models of different sizes and implementations. A description of the type of available models is in the [spaCy documentation](https://spacy.io/models/en). First, the models to be used need to be installed. We will use the following three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2JRubivyaSBx"
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download en_core_web_lg\n",
    "!python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRt0fV4tbEXt"
   },
   "source": [
    "üëã ‚öí  Use each of the three models that were downloaded above and perform named entitiy recognition with each of them on the original not preprocessed article, one after another. You can use different code cells for the different models or write everything into one cell, as you prefer. For each of the model outputs, automatically calculate the number of NERs for each NER type that the model identifies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZpLgXnJxbMCf"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp_sm = spacy.load(\"en_core_web_sm\")\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LDM8iLRNcdKo"
   },
   "source": [
    "You can use the following function to visualize the named entities in the text in order to facilitate the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3CWq0ejhceIH"
   },
   "outputs": [],
   "source": [
    "# You can also visualize the detected named entities\n",
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "of4jzY-JdA0b"
   },
   "source": [
    "üëã ‚öí Add your text of the analysis of differences between the three different models right here in the next text field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KwDuP5aDdCN2"
   },
   "source": [
    "*Your NE performance analysis here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CqztuZBcrw7"
   },
   "source": [
    "üëã ‚öí Compare the analysis of the best performing spaCy model for NER on the article after it was preprocessed to the performance on the non-preprocessed article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2cxBSOWmc-5x"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "enZaik7ZdPYm"
   },
   "source": [
    "## **Multilingual NER**\n",
    "In this exercise, the NER performance of spaCy in English is compared to another language of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8DFV273eDBN"
   },
   "source": [
    "üëã ‚öí Go the [spaCy page](https://spacy.io/models) detailing the available models to identify supported languages on the left listed under the heading \"Trained Pipelines\". Select a language and model of your choice. Find an article in this language and parse it using the newspaper package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o3NW0KRYeCPs"
   },
   "outputs": [],
   "source": [
    "# Remember that you first need to load the model by replacing\n",
    "#\"en_core_web_sm\" with the name of your model\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-KaoiUwqeLSC"
   },
   "source": [
    "üëã ‚öí Perform NER on the selected article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AeDyii4SeT3-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFeUoLxheXao"
   },
   "source": [
    "üëã ‚öí How well did the NER in the language of your choice work as compared to the overall performance of NER with spaCy in English?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BURacTomelzy"
   },
   "source": [
    "*Your NE performance analysis here*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
